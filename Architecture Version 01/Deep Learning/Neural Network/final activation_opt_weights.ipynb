{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7de05a23",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81982f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75517e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emg1</th>\n",
       "      <th>emg2</th>\n",
       "      <th>emg3</th>\n",
       "      <th>emg4</th>\n",
       "      <th>emg5</th>\n",
       "      <th>emg6</th>\n",
       "      <th>emg7</th>\n",
       "      <th>emg8</th>\n",
       "      <th>emg9</th>\n",
       "      <th>emg10</th>\n",
       "      <th>repetition</th>\n",
       "      <th>rerepetition</th>\n",
       "      <th>stimulus</th>\n",
       "      <th>restimulus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29443</th>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0146</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3965</th>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.1636</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.1831</td>\n",
       "      <td>0.0635</td>\n",
       "      <td>0.0586</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31703</th>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0293</td>\n",
       "      <td>0.0122</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.5493</td>\n",
       "      <td>0.0879</td>\n",
       "      <td>0.0977</td>\n",
       "      <td>0.0439</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24417</th>\n",
       "      <td>0.0122</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0684</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54360</th>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.3735</td>\n",
       "      <td>0.0928</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0928</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         emg1    emg2    emg3    emg4    emg5    emg6    emg7    emg8    emg9  \\\n",
       "29443  0.0024  0.0024  0.0024  0.0024  0.0024  0.0024  0.0024  0.0244  0.0024   \n",
       "3965   0.0049  0.1636  0.0024  0.0024  0.0049  0.0024  0.1831  0.0635  0.0586   \n",
       "31703  0.0024  0.0293  0.0122  0.0024  0.0024  0.0024  0.5493  0.0879  0.0977   \n",
       "24417  0.0122  0.0024  0.0024  0.0024  0.0024  0.0024  0.0684  0.1099  0.0024   \n",
       "54360  0.0049  0.0024  0.0024  0.0024  0.0024  0.0024  0.3735  0.0928  0.0024   \n",
       "\n",
       "        emg10  repetition  rerepetition  stimulus  restimulus  \n",
       "29443  0.0146           0             0         0           0  \n",
       "3965   0.0073           5             5         1           1  \n",
       "31703  0.0439           8             8         4           4  \n",
       "24417  0.0024           0             0         0           0  \n",
       "54360  0.0928           5             0         7           0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = pd.read_excel('Dataset1 .xlsx')\n",
    "raw_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f837e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_names = raw_data[ (raw_data['stimulus'] != raw_data['restimulus'])].index\n",
    "raw_data.drop(index_names, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d7278cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_names = raw_data[ (raw_data['repetition'] != raw_data['rerepetition'])].index\n",
    "raw_data.drop(index_names, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "771952e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = raw_data.iloc[:,0:10]\n",
    "y = raw_data.stimulus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c8c6acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras import activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd0c5c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3b321b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding for categorical labels\n",
    "import keras\n",
    "y = keras.utils.to_categorical(y, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd90e9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model\n",
    "input_dim = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e1beac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "standardscaler=StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4413f1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pd.DataFrame(standardscaler.fit_transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a37aac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.124694</td>\n",
       "      <td>-0.428949</td>\n",
       "      <td>-0.447634</td>\n",
       "      <td>-0.376741</td>\n",
       "      <td>-0.171824</td>\n",
       "      <td>0.139046</td>\n",
       "      <td>-0.439076</td>\n",
       "      <td>-0.441237</td>\n",
       "      <td>-0.537378</td>\n",
       "      <td>-0.250667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.057307</td>\n",
       "      <td>-0.428949</td>\n",
       "      <td>-0.447634</td>\n",
       "      <td>-0.376741</td>\n",
       "      <td>-0.171824</td>\n",
       "      <td>-0.060491</td>\n",
       "      <td>-0.439076</td>\n",
       "      <td>-0.478142</td>\n",
       "      <td>-0.537378</td>\n",
       "      <td>-0.289053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.040804</td>\n",
       "      <td>-0.428949</td>\n",
       "      <td>-0.447634</td>\n",
       "      <td>-0.376741</td>\n",
       "      <td>-0.171824</td>\n",
       "      <td>-0.060491</td>\n",
       "      <td>-0.439076</td>\n",
       "      <td>-0.490275</td>\n",
       "      <td>-0.537378</td>\n",
       "      <td>-0.327440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.040804</td>\n",
       "      <td>-0.428949</td>\n",
       "      <td>-0.447634</td>\n",
       "      <td>-0.376741</td>\n",
       "      <td>-0.171824</td>\n",
       "      <td>-0.060491</td>\n",
       "      <td>-0.439076</td>\n",
       "      <td>-0.515047</td>\n",
       "      <td>-0.537378</td>\n",
       "      <td>-0.384629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.010081</td>\n",
       "      <td>-0.428949</td>\n",
       "      <td>-0.447634</td>\n",
       "      <td>-0.376741</td>\n",
       "      <td>-0.171824</td>\n",
       "      <td>-0.162295</td>\n",
       "      <td>-0.439076</td>\n",
       "      <td>-0.502914</td>\n",
       "      <td>-0.537378</td>\n",
       "      <td>-0.404214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75038</th>\n",
       "      <td>-0.329139</td>\n",
       "      <td>-0.428949</td>\n",
       "      <td>-0.447634</td>\n",
       "      <td>-0.376741</td>\n",
       "      <td>0.048955</td>\n",
       "      <td>-0.162295</td>\n",
       "      <td>-0.439076</td>\n",
       "      <td>-0.502914</td>\n",
       "      <td>-0.537378</td>\n",
       "      <td>-0.499790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75039</th>\n",
       "      <td>-0.329139</td>\n",
       "      <td>-0.428949</td>\n",
       "      <td>-0.447634</td>\n",
       "      <td>-0.376741</td>\n",
       "      <td>-0.171824</td>\n",
       "      <td>-0.162295</td>\n",
       "      <td>-0.439076</td>\n",
       "      <td>-0.515047</td>\n",
       "      <td>-0.537378</td>\n",
       "      <td>-0.499790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75040</th>\n",
       "      <td>-0.329139</td>\n",
       "      <td>-0.428949</td>\n",
       "      <td>-0.447634</td>\n",
       "      <td>-0.376741</td>\n",
       "      <td>-0.171824</td>\n",
       "      <td>-0.162295</td>\n",
       "      <td>-0.439076</td>\n",
       "      <td>-0.515047</td>\n",
       "      <td>-0.537378</td>\n",
       "      <td>-0.461403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75041</th>\n",
       "      <td>-0.329139</td>\n",
       "      <td>-0.428949</td>\n",
       "      <td>-0.447634</td>\n",
       "      <td>-0.376741</td>\n",
       "      <td>-0.171824</td>\n",
       "      <td>-0.162295</td>\n",
       "      <td>-0.439076</td>\n",
       "      <td>-0.502914</td>\n",
       "      <td>-0.537378</td>\n",
       "      <td>-0.461403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75042</th>\n",
       "      <td>-0.329139</td>\n",
       "      <td>-0.428949</td>\n",
       "      <td>-0.447634</td>\n",
       "      <td>-0.376741</td>\n",
       "      <td>-0.171824</td>\n",
       "      <td>-0.162295</td>\n",
       "      <td>-0.439076</td>\n",
       "      <td>-0.490275</td>\n",
       "      <td>-0.537378</td>\n",
       "      <td>-0.499790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75043 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6  \\\n",
       "0      0.124694 -0.428949 -0.447634 -0.376741 -0.171824  0.139046 -0.439076   \n",
       "1      0.057307 -0.428949 -0.447634 -0.376741 -0.171824 -0.060491 -0.439076   \n",
       "2      0.040804 -0.428949 -0.447634 -0.376741 -0.171824 -0.060491 -0.439076   \n",
       "3      0.040804 -0.428949 -0.447634 -0.376741 -0.171824 -0.060491 -0.439076   \n",
       "4     -0.010081 -0.428949 -0.447634 -0.376741 -0.171824 -0.162295 -0.439076   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "75038 -0.329139 -0.428949 -0.447634 -0.376741  0.048955 -0.162295 -0.439076   \n",
       "75039 -0.329139 -0.428949 -0.447634 -0.376741 -0.171824 -0.162295 -0.439076   \n",
       "75040 -0.329139 -0.428949 -0.447634 -0.376741 -0.171824 -0.162295 -0.439076   \n",
       "75041 -0.329139 -0.428949 -0.447634 -0.376741 -0.171824 -0.162295 -0.439076   \n",
       "75042 -0.329139 -0.428949 -0.447634 -0.376741 -0.171824 -0.162295 -0.439076   \n",
       "\n",
       "              7         8         9  \n",
       "0     -0.441237 -0.537378 -0.250667  \n",
       "1     -0.478142 -0.537378 -0.289053  \n",
       "2     -0.490275 -0.537378 -0.327440  \n",
       "3     -0.515047 -0.537378 -0.384629  \n",
       "4     -0.502914 -0.537378 -0.404214  \n",
       "...         ...       ...       ...  \n",
       "75038 -0.502914 -0.537378 -0.499790  \n",
       "75039 -0.515047 -0.537378 -0.499790  \n",
       "75040 -0.515047 -0.537378 -0.461403  \n",
       "75041 -0.502914 -0.537378 -0.461403  \n",
       "75042 -0.490275 -0.537378 -0.499790  \n",
       "\n",
       "[75043 rows x 10 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53413956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(sc, y, test_size = 0.2, random_state = 39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "577a1d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0609375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from math import floor\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.layers import LeakyReLU\n",
    "LeakyReLU = LeakyReLU(alpha=0.1)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94bfb058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c021a414",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1222eb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(activation='relu',optimizer='Adam'):   \n",
    "    \n",
    "    \n",
    "    visible = Input(shape=(input_dim,))\n",
    "    hidden1 = Dense(10, activation=activation)(visible)\n",
    "    hidden2 = Dense(20, activation=activation)(hidden1)\n",
    "    hidden3 = Dense(10, activation=activation)(hidden2)\n",
    "    output = Dense(num_classes, activation=activation)(hidden3)\n",
    "    model = Model(inputs=visible, outputs=output)\n",
    "    \n",
    "    \n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',optimizer=optimizer,metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b803e666",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "batch_size = 10\n",
    "epochs = 10\n",
    "\n",
    "model = KerasClassifier(build_fn=define_model, epochs=epochs, batch_size = batch_size)\n",
    "\n",
    "activation = ['softmax', 'relu']\n",
    "#Also try softplus, tanh, linear, hard_sigmoid \n",
    "optimizer = ['SGD', 'RMSprop', 'Adam']\n",
    "\n",
    "\n",
    "#Cross validation, cv=3\n",
    "param_grid = dict(activation=activation,optimizer=optimizer)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid,cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2454c34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4003/4003 [==============================] - 4s 990us/step - loss: 1.9422 - acc: 0.5072\n",
      "Epoch 2/10\n",
      "4003/4003 [==============================] - 4s 1ms/step - loss: 1.9040 - acc: 0.5105A: 0s - loss:\n",
      "Epoch 3/10\n",
      "4003/4003 [==============================] - 4s 1ms/step - loss: 1.9038 - acc: 0.5105A: 0s - loss: 1.9015 - acc: \n",
      "Epoch 4/10\n",
      "4003/4003 [==============================] - 5s 1ms/step - loss: 1.9039 - acc: 0.5105\n",
      "Epoch 5/10\n",
      "4003/4003 [==============================] - 4s 1ms/step - loss: 1.9039 - acc: 0.5105\n",
      "Epoch 6/10\n",
      "4003/4003 [==============================] - 5s 1ms/step - loss: 1.9038 - acc: 0.5105\n",
      "Epoch 7/10\n",
      "4003/4003 [==============================] - 4s 948us/step - loss: 1.9038 - acc: 0.5105\n",
      "Epoch 8/10\n",
      "4003/4003 [==============================] - 6s 1ms/step - loss: 1.9038 - acc: 0.5105\n",
      "Epoch 9/10\n",
      "4003/4003 [==============================] - 6s 2ms/step - loss: 1.9037 - acc: 0.5105A: 4s - loss: 1.9 - ETA - ETA: 0s - loss: 1.9009 - acc: 0\n",
      "Epoch 10/10\n",
      "4003/4003 [==============================] - 6s 2ms/step - loss: 1.9038 - acc: 0.5105A: 1s - l\n",
      "2002/2002 [==============================] - 2s 1ms/step - loss: 1.9163 - acc: 0.5059A:\n",
      "Epoch 1/10\n",
      "4003/4003 [==============================] - 4s 1ms/step - loss: 1.9484 - acc: 0.5042\n",
      "Epoch 2/10\n",
      "4003/4003 [==============================] - 4s 994us/step - loss: 1.9060 - acc: 0.5099\n",
      "Epoch 3/10\n",
      "4003/4003 [==============================] - 4s 959us/step - loss: 1.9061 - acc: 0.5099\n",
      "Epoch 4/10\n",
      "4003/4003 [==============================] - 5s 1ms/step - loss: 1.9060 - acc: 0.5099\n",
      "Epoch 5/10\n",
      "4003/4003 [==============================] - 4s 1ms/step - loss: 1.9059 - acc: 0.5099\n",
      "Epoch 6/10\n",
      "4003/4003 [==============================] - 5s 1ms/step - loss: 1.9060 - acc: 0.5099\n",
      "Epoch 7/10\n",
      "4003/4003 [==============================] - 5s 1ms/step - loss: 1.9060 - acc: 0.5099A: 1s - loss: 1.9019 - acc: - E\n",
      "Epoch 8/10\n",
      "4003/4003 [==============================] - 6s 1ms/step - loss: 1.9059 - acc: 0.5099A: 0s - loss: 1.9034 - acc: 0 - ETA: 0s - loss: 1.9027 - \n",
      "Epoch 9/10\n",
      "4003/4003 [==============================] - 5s 1ms/step - loss: 1.9059 - acc: 0.5099\n",
      "Epoch 10/10\n",
      "4003/4003 [==============================] - 5s 1ms/step - loss: 1.9058 - acc: 0.5099\n",
      "2002/2002 [==============================] - 2s 878us/step - loss: 1.9117 - acc: 0.5072\n",
      "Epoch 1/10\n",
      "4003/4003 [==============================] - 5s 1ms/step - loss: 1.9535 - acc: 0.5020A: 1s - loss: 1.9744 - acc: - ETA\n",
      "Epoch 2/10\n",
      "4003/4003 [==============================] - 5s 1ms/step - loss: 1.9141 - acc: 0.5066A: 1s - loss:  - ETA: 0s - loss: 1.9171 - ETA: 0s - loss: 1.9144 - acc: 0.\n",
      "Epoch 3/10\n",
      "4003/4003 [==============================] - 6s 2ms/step - loss: 1.9141 - acc: 0.5066A: 1s - loss: 1.9157 - - ETA: 0s - loss: 1.9151\n",
      "Epoch 4/10\n",
      "4003/4003 [==============================] - 5s 1ms/step - loss: 1.9141 - acc: 0.5066\n",
      "Epoch 5/10\n",
      "4003/4003 [==============================] - 4s 1ms/step - loss: 1.9140 - acc: 0.5066\n",
      "Epoch 6/10\n",
      "4003/4003 [==============================] - 5s 1ms/step - loss: 1.9140 - acc: 0.5066\n",
      "Epoch 7/10\n",
      "4003/4003 [==============================] - 4s 1ms/step - loss: 1.9140 - acc: 0.5066\n",
      "Epoch 8/10\n",
      "4003/4003 [==============================] - 4s 1ms/step - loss: 1.9140 - acc: 0.5066\n",
      "Epoch 9/10\n",
      "4003/4003 [==============================] - 5s 1ms/step - loss: 1.9140 - acc: 0.5066\n",
      "Epoch 10/10\n",
      "4003/4003 [==============================] - 6s 1ms/step - loss: 1.9140 - acc: 0.5066\n",
      "2002/2002 [==============================] - 2s 999us/step - loss: 1.8960 - acc: 0.5138\n",
      "Epoch 1/10\n",
      "4003/4003 [==============================] - 5s 1ms/step - loss: 1.7678 - acc: 0.5128\n",
      "Epoch 2/10\n",
      "4003/4003 [==============================] - 4s 1ms/step - loss: 1.4286 - acc: 0.5643\n",
      "Epoch 3/10\n",
      "4003/4003 [==============================] - 4s 1ms/step - loss: 1.2898 - acc: 0.6007A: 1s - lo\n",
      "Epoch 4/10\n",
      "4003/4003 [==============================] - 4s 1ms/step - loss: 1.2324 - acc: 0.6139\n",
      "Epoch 5/10\n",
      "4003/4003 [==============================] - 4s 1ms/step - loss: 1.1880 - acc: 0.6139\n",
      "Epoch 6/10\n",
      "4003/4003 [==============================] - 5s 1ms/step - loss: 1.1400 - acc: 0.6138A:\n",
      "Epoch 7/10\n",
      "4003/4003 [==============================] - 6s 1ms/step - loss: 1.1021 - acc: 0.6372\n",
      "Epoch 8/10\n",
      "4003/4003 [==============================] - 6s 1ms/step - loss: 1.0639 - acc: 0.6520\n",
      "Epoch 9/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: 1.0246 - acc: 0.6681\n",
      "Epoch 10/10\n",
      "4003/4003 [==============================] - 8s 2ms/step - loss: 0.9907 - acc: 0.6791A: 2s - los - ETA: 0s - loss: \n",
      "2002/2002 [==============================] - 3s 2ms/step - loss: 0.9602 - acc: 0.6863\n",
      "Epoch 1/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: 1.7754 - acc: 0.5135 - ETA: 2s - loss: 1.8890 - acc: 0.5 - ETA: 2s - loss: 1.8802 - acc - ETA\n",
      "Epoch 2/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: 1.4571 - acc: 0.5421\n",
      "Epoch 3/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: 1.3341 - acc: 0.5885A: 6s - ETA - ETA: 1s - loss: 1.3521 - acc: 0 - ETA: 1s - loss: 1 - ETA: 0s - loss: 1.3350 - acc: 0.58\n",
      "Epoch 4/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: 1.2351 - acc: 0.6068\n",
      "Epoch 5/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: 1.1675 - acc: 0.6259A: 1s \n",
      "Epoch 6/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: 1.1279 - acc: 0.6415\n",
      "Epoch 7/10\n",
      "4003/4003 [==============================] - 6s 1ms/step - loss: 1.0913 - acc: 0.6584A: 1s\n",
      "Epoch 8/10\n",
      "4003/4003 [==============================] - ETA: 0s - loss: 1.0448 - acc: 0.6801- ETA: 0s - loss: 1.0503  - 7s 2ms/step - loss: 1.0449 - acc: 0.6803\n",
      "Epoch 9/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: 0.9950 - acc: 0.6971A: 1s - loss: 1.0026  - ETA: 1s - los\n",
      "Epoch 10/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: 0.9525 - acc: 0.7171\n",
      "2002/2002 [==============================] - 3s 1ms/step - loss: 0.9544 - acc: 0.7261\n",
      "Epoch 1/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: 1.7874 - acc: 0.5082A: 3s - loss: 1.9659 - acc: 0 - ETA: 2s - loss: 1.94 - ETA: 0s - loss: 1.7881 - acc: 0.508\n",
      "Epoch 2/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: 1.4484 - acc: 0.5542A: 3s - loss: 1.4737 - acc: 0. - ETA: 3s - loss: 1.4745 -  - ETA: 3s - l\n",
      "Epoch 3/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: 1.2974 - acc: 0.6047A: 0s - loss: 1.2971 - acc: 0.\n",
      "Epoch 4/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: 1.2080 - acc: 0.6252A: 5s - loss: 1.2511 - acc - ETA: 4s - loss:\n",
      "Epoch 5/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: 1.1487 - acc: 0.6348\n",
      "Epoch 6/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: 1.1244 - acc: 0.6377A: 1s - lo - ETA: 0s - loss: 1.1240 - acc: 0.637\n",
      "Epoch 7/10\n",
      "4003/4003 [==============================] - 6s 1ms/step - loss: 1.1021 - acc: 0.6409\n",
      "Epoch 8/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: 1.0657 - acc: 0.6549\n",
      "Epoch 9/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: 1.0191 - acc: 0.6762\n",
      "Epoch 10/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: 0.9888 - acc: 0.6869A: 0s - loss: 0.9890 - acc: \n",
      "2002/2002 [==============================] - 3s 1ms/step - loss: 0.9779 - acc: 0.6894\n",
      "Epoch 1/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: 1.7229 - acc: 0.5139A: 5s - ETA: 4s - loss - ETA: 1s - loss:\n",
      "Epoch 2/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: 1.4539 - acc: 0.5506A: 1s - loss: 1.4630 - acc: 0. - ETA: 1s - \n",
      "Epoch 3/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: 1.3651 - acc: 0.5730A: 5s - loss: 1. - ETA: 0s - loss: 1\n",
      "Epoch 4/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: 1.3098 - acc: 0.5824A: 0s - loss: 1.3104 - acc: 0.58\n",
      "Epoch 5/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: 1.2761 - acc: 0.5837TA: 3s - loss: 1.2889 - acc: 0.578 - ETA: 1s - loss: 1.2820 - acc: 0.581 - ETA: 1s - loss\n",
      "Epoch 6/10\n",
      "4003/4003 [==============================] - 6s 2ms/step - loss: 1.2323 - acc: 0.6071\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4003/4003 [==============================] - 8s 2ms/step - loss: 1.1631 - acc: 0.6364\n",
      "Epoch 8/10\n",
      "4003/4003 [==============================] - 8s 2ms/step - loss: 1.1273 - acc: 0.6467 - ETA: 0s - loss: 1.1266 - acc: 0.646\n",
      "Epoch 9/10\n",
      "4003/4003 [==============================] - 9s 2ms/step - loss: 1.0975 - acc: 0.6605A:  - ETA: 0s - loss: 1.0979 - acc:\n",
      "Epoch 10/10\n",
      "4003/4003 [==============================] - 8s 2ms/step - loss: 1.0599 - acc: 0.6802\n",
      "2002/2002 [==============================] - 3s 1ms/step - loss: 1.0173 - acc: 0.6931\n",
      "Epoch 1/10\n",
      "4003/4003 [==============================] - 8s 2ms/step - loss: 1.7057 - acc: 0.5161A: 1s - l\n",
      "Epoch 2/10\n",
      "4003/4003 [==============================] - 8s 2ms/step - loss: 1.4441 - acc: 0.5539A: 1s - loss: 1.4484 -  - ETA: 0s - loss: 1.4467 -\n",
      "Epoch 3/10\n",
      "4003/4003 [==============================] - 8s 2ms/step - loss: 1.3556 - acc: 0.5734\n",
      "Epoch 4/10\n",
      "4003/4003 [==============================] - 8s 2ms/step - loss: 1.2668 - acc: 0.5825A: 1s - los\n",
      "Epoch 5/10\n",
      "4003/4003 [==============================] - 8s 2ms/step - loss: 1.1996 - acc: 0.6034\n",
      "Epoch 6/10\n",
      "4003/4003 [==============================] - 8s 2ms/step - loss: 1.1640 - acc: 0.6177\n",
      "Epoch 7/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: 1.1205 - acc: 0.6402\n",
      "Epoch 8/10\n",
      "4003/4003 [==============================] - 8s 2ms/step - loss: 1.0703 - acc: 0.6556A: 1s - loss: 1.0671 -  - ETA: 0s - loss: 1.0697\n",
      "Epoch 9/10\n",
      "4003/4003 [==============================] - 8s 2ms/step - loss: 1.0253 - acc: 0.6704A: 1s - loss: 1.0 - ETA: 0s - loss: 1.02\n",
      "Epoch 10/10\n",
      "4003/4003 [==============================] - 8s 2ms/step - loss: 0.9879 - acc: 0.6912\n",
      "2002/2002 [==============================] - 3s 2ms/step - loss: 0.9877 - acc: 0.6948A: 0s - loss: 0.9891 - acc: 0.6\n",
      "Epoch 1/10\n",
      "4003/4003 [==============================] - 8s 2ms/step - loss: 1.7196 - acc: 0.5138A: 0s - loss: 1.7311 - acc:\n",
      "Epoch 2/10\n",
      "4003/4003 [==============================] - 8s 2ms/step - loss: 1.4410 - acc: 0.5578\n",
      "Epoch 3/10\n",
      "4003/4003 [==============================] - 8s 2ms/step - loss: 1.3252 - acc: 0.5913\n",
      "Epoch 4/10\n",
      "4003/4003 [==============================] - 8s 2ms/step - loss: 1.2680 - acc: 0.6051A: 4s - loss: 1.2830 - acc: 0. - ETA: 4s - loss: 1.2833\n",
      "Epoch 5/10\n",
      "4003/4003 [==============================] - 8s 2ms/step - loss: 1.2274 - acc: 0.6159 ETA: 5s - loss: 1.262 - ETA:  - ETA: 1s \n",
      "Epoch 6/10\n",
      "4003/4003 [==============================] - 8s 2ms/step - loss: 1.1622 - acc: 0.6324\n",
      "Epoch 7/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: 1.1240 - acc: 0.6398\n",
      "Epoch 8/10\n",
      "4003/4003 [==============================] - 8s 2ms/step - loss: 1.0793 - acc: 0.6663\n",
      "Epoch 9/10\n",
      "4003/4003 [==============================] - 8s 2ms/step - loss: 1.0312 - acc: 0.6742A: 1s - loss:\n",
      "Epoch 10/10\n",
      "4003/4003 [==============================] - 8s 2ms/step - loss: 0.9943 - acc: 0.6830\n",
      "2002/2002 [==============================] - 3s 1ms/step - loss: 0.9838 - acc: 0.6862A: 0s - loss: 0.9887 - acc\n",
      "Epoch 1/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: 4.2492 - acc: 0.3470\n",
      "Epoch 2/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: 4.3004 - acc: 0.0382\n",
      "Epoch 3/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: 4.1335 - acc: 0.0382A: 3s -\n",
      "Epoch 4/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: 4.0951 - acc: 0.0540A: 0s - loss: 4.0840 - a\n",
      "Epoch 5/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: 3.9287 - acc: 0.0409A: 5s - lo\n",
      "Epoch 6/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: 3.0997 - acc: 0.0613A:\n",
      "Epoch 7/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: 2.7394 - acc: 0.3632\n",
      "Epoch 8/10\n",
      "4003/4003 [==============================] - 6s 2ms/step - loss: 3.4107 - acc: 0.2328\n",
      "Epoch 9/10\n",
      "4003/4003 [==============================] - 6s 2ms/step - loss: 3.5268 - acc: 0.3719\n",
      "Epoch 10/10\n",
      "4003/4003 [==============================] - 8s 2ms/step - loss: 3.0916 - acc: 0.5191A: 0s - loss: 3.1173 - acc - ETA: 0s - loss: 3.0940 - acc: 0.518\n",
      "2002/2002 [==============================] - 3s 2ms/step - loss: 2.6959 - acc: 0.5175\n",
      "Epoch 1/10\n",
      "4003/4003 [==============================] - 8s 2ms/step - loss: 3.2223 - acc: 0.3962A: 0s - loss: 3.2494 - acc: 0\n",
      "Epoch 2/10\n",
      "4003/4003 [==============================] - 8s 2ms/step - loss: 3.0570 - acc: 0.4984\n",
      "Epoch 3/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: 3.3521 - acc: 0.5099TA: 0s - loss: 3.3551 - acc: 0.50 - ETA: 0s - loss: 3.3524 - acc: 0.50\n",
      "Epoch 4/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: 3.3392 - acc: 0.5099\n",
      "Epoch 5/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: 3.3358 - acc: 0.5099A: 0s - loss: 3.3458 - ac\n",
      "Epoch 6/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: 3.3335 - acc: 0.5099A: 4s - loss: 3.3 - ETA: 4s - loss: 3.3095 -  - ETA: 3s - - ETA: 2s - loss: 3.3311 - acc: 0.510 - ETA: 2s - loss: 3.3 - ETA: 1s - loss: 3.332 - ETA: 0s - loss: 3.3396 - acc:\n",
      "Epoch 7/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: 3.3320 - acc: 0.5099A: 3s - lo - ETA: 1s - loss: 3.3408 - ETA: 1s - loss\n",
      "Epoch 8/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: 3.3305 - acc: 0.5099A: 0s - loss: 3.3\n",
      "Epoch 9/10\n",
      "4003/4003 [==============================] - 5s 1ms/step - loss: 3.3286 - acc: 0.5099\n",
      "Epoch 10/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: 3.3266 - acc: 0.5099A: 1s - loss: 3.3 - ETA: 0s - loss: 3.3274 -\n",
      "2002/2002 [==============================] - 3s 2ms/step - loss: 3.3421 - acc: 0.5072\n",
      "Epoch 1/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: 8.8328 - acc: 0.0982A: 4s - loss: 9.4415 - \n",
      "Epoch 2/10\n",
      "4003/4003 [==============================] - 8s 2ms/step - loss: 4.0303 - acc: 0.1424 - E\n",
      "Epoch 3/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: 4.4435 - acc: 0.0349A: 3s - loss: 4.5156\n",
      "Epoch 4/10\n",
      "4003/4003 [==============================] - 8s 2ms/step - loss: 4.2797 - acc: 0.0349\n",
      "Epoch 5/10\n",
      "4003/4003 [==============================] - 6s 2ms/step - loss: 4.1492 - acc: 0.2204\n",
      "Epoch 6/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: 3.9877 - acc: 0.5066\n",
      "Epoch 7/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: 3.8713 - acc: 0.5066A: 1s\n",
      "Epoch 8/10\n",
      "4003/4003 [==============================] - 6s 1ms/step - loss: 3.8635 - acc: 0.5066A: 0s - loss: 3.8659 - acc: 0.\n",
      "Epoch 9/10\n",
      "4003/4003 [==============================] - 6s 2ms/step - loss: 3.8611 - acc: 0.5066A: 2s - los - ETA: 0s - loss: 3.8529 - a\n",
      "Epoch 10/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: 3.8590 - acc: 0.5066\n",
      "2002/2002 [==============================] - 3s 2ms/step - loss: 3.7944 - acc: 0.5138A: 1s - loss: 3.8174 -  - ETA: 1s - loss: 3.7923  - ETA: 0s - loss: 3.8192 - acc: 0. - ETA: 0s - loss: 3.7992 - acc: 0.5 - ETA: 0s - loss: 3.8043 - acc\n",
      "Epoch 1/10\n",
      "4003/4003 [==============================] - 8s 2ms/step - loss: nan - acc: 0.4269: 0s - los\n",
      "Epoch 2/10\n",
      "4003/4003 [==============================] - 9s 2ms/step - loss: nan - acc: 0.5105:  - E\n",
      "Epoch 3/10\n",
      "4003/4003 [==============================] - 8s 2ms/step - loss: nan - acc: 0.5105: 1s -\n",
      "Epoch 4/10\n",
      "4003/4003 [==============================] - 8s 2ms/step - loss: nan - acc: 0.5105: 0s - l\n",
      "Epoch 5/10\n",
      "4003/4003 [==============================] - ETA: 0s - loss: nan - acc: 0.5108 - ETA: 0s - loss: nan - acc - 8s 2ms/step - loss: nan - acc: 0.5105\n",
      "Epoch 6/10\n",
      "4003/4003 [==============================] - 8s 2ms/step - loss: nan - acc: 0.5105\n",
      "Epoch 7/10\n",
      "4003/4003 [==============================] - 8s 2ms/step - loss: nan - acc: 0.5105\n",
      "Epoch 8/10\n",
      "4003/4003 [==============================] - 8s 2ms/step - loss: nan - acc: 0.5105\n",
      "Epoch 9/10\n",
      "4003/4003 [==============================] - 8s 2ms/step - loss: nan - acc: 0.5105: 0s - loss: nan - acc\n",
      "Epoch 10/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: nan - acc: 0.5105: 0s - loss: nan - acc: 0.51\n",
      "2002/2002 [==============================] - 3s 1ms/step - loss: nan - acc: 0.5059\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4003/4003 [==============================] - 7s 2ms/step - loss: nan - acc: 0.5144\n",
      "Epoch 2/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: nan - acc: 0.5099: \n",
      "Epoch 3/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: nan - acc: 0.5099\n",
      "Epoch 4/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: nan - acc: 0.5099: 0s - - ETA: 0s - loss: nan - acc: 0.50\n",
      "Epoch 5/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: nan - acc: 0.5099: 0s - l\n",
      "Epoch 6/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: nan - acc: 0.5099: 0s - loss: nan - acc: 0.50\n",
      "Epoch 7/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: nan - acc: 0.5099: 0s - loss: nan - acc:  - ETA: 0s\n",
      "Epoch 8/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: nan - acc: 0.5099\n",
      "Epoch 9/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: nan - acc: 0.5099\n",
      "Epoch 10/10\n",
      "4003/4003 [==============================] - 6s 1ms/step - loss: nan - acc: 0.5099\n",
      "2002/2002 [==============================] - 3s 1ms/step - loss: nan - acc: 0.5072: 0s\n",
      "Epoch 1/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: nan - acc: 0.5073: 0s - loss: nan - acc: 0.50\n",
      "Epoch 2/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: nan - acc: 0.5066\n",
      "Epoch 3/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: nan - acc: 0.5066\n",
      "Epoch 4/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: nan - acc: 0.5066 ETA: \n",
      "Epoch 5/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: nan - acc: 0.5066\n",
      "Epoch 6/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: nan - acc: 0.5066\n",
      "Epoch 7/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: nan - acc: 0.5066\n",
      "Epoch 8/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: nan - acc: 0.5066: 0s - loss: nan\n",
      "Epoch 9/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: nan - acc: 0.5066 ETA: 0s - loss: nan - acc: 0.50\n",
      "Epoch 10/10\n",
      "4003/4003 [==============================] - 6s 2ms/step - loss: nan - acc: 0.5066\n",
      "2002/2002 [==============================] - ETA: 0s - loss: nan - acc: 0.5136 ETA: 1s - los - ETA: 0s - l - 3s 1ms/step - loss: nan - acc: 0.5138\n",
      "Epoch 1/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: nan - acc: 0.3542: 0s - los\n",
      "Epoch 2/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: nan - acc: 0.5105\n",
      "Epoch 3/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: nan - acc: 0.5105\n",
      "Epoch 4/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: nan - acc: 0.5105: 3s - loss: n - - ETA: 0s - loss: nan - acc: 0.51\n",
      "Epoch 5/10\n",
      "4003/4003 [==============================] - 8s 2ms/step - loss: nan - acc: 0.5105: 0s -\n",
      "Epoch 6/10\n",
      "4003/4003 [==============================] - 8s 2ms/step - loss: nan - acc: 0.5105\n",
      "Epoch 7/10\n",
      "4003/4003 [==============================] - 8s 2ms/step - loss: nan - acc: 0.5105\n",
      "Epoch 8/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: nan - acc: 0.5105\n",
      "Epoch 9/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: nan - acc: 0.5105\n",
      "Epoch 10/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: nan - acc: 0.5105\n",
      "2002/2002 [==============================] - 3s 1ms/step - loss: nan - acc: 0.5059\n",
      "Epoch 1/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: 2.3001 - acc: 0.5982\n",
      "Epoch 2/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: nan - acc: 0.5413\n",
      "Epoch 3/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: nan - acc: 0.5099: 0s - loss: nan - acc: 0.\n",
      "Epoch 4/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: nan - acc: 0.5099\n",
      "Epoch 5/10\n",
      "4003/4003 [==============================] - 8s 2ms/step - loss: nan - acc: 0.5099\n",
      "Epoch 6/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: nan - acc: 0.5099\n",
      "Epoch 7/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: nan - acc: 0.5099\n",
      "Epoch 8/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: nan - acc: 0.5099\n",
      "Epoch 9/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: nan - acc: 0.5099: 2s - loss: nan - ETA: \n",
      "Epoch 10/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: nan - acc: 0.5099\n",
      "2002/2002 [==============================] - 3s 1ms/step - loss: nan - acc: 0.5072\n",
      "Epoch 1/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: nan - acc: 0.4237\n",
      "Epoch 2/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: nan - acc: 0.5066\n",
      "Epoch 3/10\n",
      "4003/4003 [==============================] - 8s 2ms/step - loss: nan - acc: 0.5066\n",
      "Epoch 4/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: nan - acc: 0.5066\n",
      "Epoch 5/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: nan - acc: 0.5066\n",
      "Epoch 6/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: nan - acc: 0.5066\n",
      "Epoch 7/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: nan - acc: 0.5066: 0s - loss: nan - acc - ETA: 0s - loss: nan - acc: \n",
      "Epoch 8/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: nan - acc: 0.5066\n",
      "Epoch 9/10\n",
      "4003/4003 [==============================] - 7s 2ms/step - loss: nan - acc: 0.5066: \n",
      "Epoch 10/10\n",
      "4003/4003 [==============================] - 8s 2ms/step - loss: nan - acc: 0.5066: 1s - loss: nan - acc:  - ETA - ETA: \n",
      "2002/2002 [==============================] - 3s 1ms/step - loss: nan - acc: 0.5138\n",
      "Epoch 1/10\n",
      "6004/6004 [==============================] - 10s 2ms/step - loss: 1.6803 - acc: 0.5164\n",
      "Epoch 2/10\n",
      "6004/6004 [==============================] - ETA: 0s - loss: 1.2938 - acc: 0.5927- ETA: 0s - loss: 1.2969 - acc: 0 - 10s 2ms/step - loss: 1.2933 - acc: 0.5928\n",
      "Epoch 3/10\n",
      "6004/6004 [==============================] - 10s 2ms/step - loss: 1.1794 - acc: 0.6155: 9s - loss: 1.19 \n",
      "Epoch 4/10\n",
      "6004/6004 [==============================] - 10s 2ms/step - loss: 1.1233 - acc: 0.6430\n",
      "Epoch 5/10\n",
      "6004/6004 [==============================] - 9s 2ms/step - loss: 1.0528 - acc: 0.6535\n",
      "Epoch 6/10\n",
      "6004/6004 [==============================] - 11s 2ms/step - loss: 1.0112 - acc: 0.6662\n",
      "Epoch 7/10\n",
      "6004/6004 [==============================] - 10s 2ms/step - loss: 0.9805 - acc: 0.6878: 0s - loss: 0.98\n",
      "Epoch 8/10\n",
      "6004/6004 [==============================] - 10s 2ms/step - loss: 0.9491 - acc: 0.7060\n",
      "Epoch 9/10\n",
      "6004/6004 [==============================] - 10s 2ms/step - loss: 0.9197 - acc: 0.7170: 2s - loss: 0.9175 -  - ETA:\n",
      "Epoch 10/10\n",
      "6004/6004 [==============================] - ETA: 0s - loss: 0.8969 - acc: 0.7251- ETA: 2s - loss: 0.8 - ETA:  - 10s 2ms/step - loss: 0.8969 - acc: 0.7251\n"
     ]
    }
   ],
   "source": [
    "grid_result = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a27798af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.700603 using {'activation': 'softmax', 'optimizer': 'RMSprop'}\n",
      "Mean = 0.508978 (std=0.003464) with: {'activation': 'softmax', 'optimizer': 'SGD'}\n",
      "Mean = 0.700603 (std=0.018075) with: {'activation': 'softmax', 'optimizer': 'RMSprop'}\n",
      "Mean = 0.691342 (std=0.003719) with: {'activation': 'softmax', 'optimizer': 'Adam'}\n",
      "Mean = 0.512859 (std=0.004267) with: {'activation': 'relu', 'optimizer': 'SGD'}\n",
      "Mean = 0.508978 (std=0.003464) with: {'activation': 'relu', 'optimizer': 'RMSprop'}\n",
      "Mean = 0.508978 (std=0.003464) with: {'activation': 'relu', 'optimizer': 'Adam'}\n"
     ]
    }
   ],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"Mean = %f (std=%f) with: %r\" % (mean, stdev, param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce53a40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606983d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e55f9b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
